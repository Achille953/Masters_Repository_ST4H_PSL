{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "f0cabcad77a2253757f400696d74a8a30c2e4cc9",
        "id": "5jmtqQTNi_Nk"
      },
      "source": [
        "# **CNN for classification of skin cancer images**\n",
        "\n",
        "**Goal**: Train a CNN model to classify 7 types of skin cancer based on dermatology images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnivyB-jh4zH"
      },
      "source": [
        "# ðŸ“‚ **Data Access & Setup**\n",
        "\n",
        "This project uses **HAM10000 (\"Human Against Machine with 10000 training images\")** dataset\n",
        "\n",
        "It consists of 10015 dermatoscopic images which are released as a training set for academic machine learning purposes and are publicly available through the ISIC archive. This benchmark dataset can be used for machine learning and for comparisons with human experts.\n",
        "\n",
        "It has 7 different classes of skin cancer which are listed below:\n",
        "\n",
        "**1. Melanocytic nevi <br>**\n",
        "**2. Melanoma <br>**\n",
        "**3. Benign keratosis-like lesions <br>**\n",
        "**4. Basal cell carcinoma <br>**\n",
        "**5. Actinic keratoses <br>**\n",
        "**6. Vascular lesions <br>**\n",
        "**7. Dermatofibroma<br>**\n",
        "\n",
        "We will follow these steps to classify moles into 7 classes.\n",
        "\n",
        "**1. Project Configuration**<br>\n",
        "**2. Making Dictionary of labels** <br>\n",
        "**3. Reading and Processing Data** <br>\n",
        "**4. Exploratory data analysis (EDA)** <br>\n",
        "**5. Loading the images** <br>\n",
        "**6. Dataset Preparation**<br>\n",
        "**7. Normalization**<br>\n",
        "**8. Train validation split** <br>\n",
        "**9. Model Building (CNN)** <br>\n",
        "**10. Training Configuration** <br>\n",
        "**11. Data Augmentation** <br>\n",
        "**12. Training the model**<br>\n",
        "**13. Model Evaluation** <br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnliyoJm6yMB"
      },
      "source": [
        "## 1. Project Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQzaGiezi_No",
        "outputId": "104a1b2b-d34e-4ea1-a5ee-2f9162a20da9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os, sys, gdown, tarfile, io, copy\n",
        "\n",
        "from PIL import Image\n",
        "from time import time\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "np.random.seed(123) # fix random seed\n",
        "\n",
        "print('PyTorch version: {}'.format(torch.__version__))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1zBdNFK0aOG",
        "outputId": "0b95229e-cc53-4f41-feef-ba8f6180a5a0"
      },
      "outputs": [],
      "source": [
        "# Create the exercise_functions.py file\n",
        "\n",
        "exercise_functions_code = '''\n",
        "\n",
        "def plot_training_history(history):\n",
        "    \"\"\"Plot training and validation loss and accuracy\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Plot loss\n",
        "    ax1.plot(history['train_loss'], label='Train Loss')\n",
        "    ax1.plot(history['val_loss'], label='Validation Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.set_title('Training and Validation Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Plot accuracy\n",
        "    ax2.plot(history['train_acc'], label='Train Accuracy')\n",
        "    ax2.plot(history['val_acc'], label='Validation Accuracy')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.set_title('Training and Validation Accuracy')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, classes):\n",
        "    \"\"\"Plot confusion matrix\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=classes, yticklabels=classes)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def get_device():\n",
        "    \"\"\"Get the device (GPU if available, else CPU)\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f'Using device: {device}')\n",
        "    return device\n",
        "\n",
        "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    \"\"\"Validate the model\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "'''\n",
        "\n",
        "# Determine where to save the file\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    # If in Colab, save to the Colab Notebooks directory\n",
        "    save_path = ''  # Write your path\n",
        "except:\n",
        "    # If local, save to current directory\n",
        "    save_path = 'exercise_functions.py'\n",
        "\n",
        "# Write the file\n",
        "with open(save_path, 'w') as f:\n",
        "    f.write(exercise_functions_code)\n",
        "\n",
        "print(f'âœ“ Created exercise_functions.py at: {save_path}')\n",
        "print('âœ“ You can now run your original code without errors!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QvG_dwh6yMF"
      },
      "source": [
        "Check if a GPU device is available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMnzz0cE6yMG",
        "outputId": "52afab2d-89da-43c3-bc64-2618f5272e6e"
      },
      "outputs": [],
      "source": [
        "if not torch.cuda.is_available():\n",
        "    raise SystemError('No GPU device found')\n",
        "\n",
        "num_device = torch.cuda.current_device()\n",
        "device = torch.cuda.device(num_device)\n",
        "print('GPU device found. Current: [{}] {}'.format(num_device, device))\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6_sE4LOr0MV"
      },
      "source": [
        "**Your data will be stored in your google drive space. You need to mount your drive to access it from the notebook.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5QuTyFfrrM7",
        "outputId": "27aaf21b-00c5-4799-dc23-c3d7270200c4"
      },
      "outputs": [],
      "source": [
        "project_folder_name = 'Skin_Cancer_class'\n",
        "\n",
        "base_working_dir = '/content/drive/My Drive/Colab Notebooks'\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except:\n",
        "    base_working_dir = os.getcwd()\n",
        "    pass\n",
        "\n",
        "sys.path.append(base_working_dir)\n",
        "from exercise_functions import *\n",
        "\n",
        "base_working_dir = os.path.join(base_working_dir, project_folder_name)\n",
        "\n",
        "if not os.path.exists(base_working_dir):\n",
        "    os.makedirs(base_working_dir)\n",
        "\n",
        "print('Working directory: {}'.format(base_working_dir))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "c170def1ed6bd1e279dc6d5ae86a95cf6cfd2efb",
        "id": "J78Y6OhRi_Nx"
      },
      "source": [
        "# 2. Making Dictionary of labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "vdjg_1sLi_Ny",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "lesion_type_dict = {\n",
        "    'nv': 'Melanocytic nevi',\n",
        "    'mel': 'Melanoma',\n",
        "    'bkl': 'Benign keratosis-like lesions ',\n",
        "    'bcc': 'Basal cell carcinoma',\n",
        "    'akiec': 'Actinic keratoses',\n",
        "    'vasc': 'Vascular lesions',\n",
        "    'df': 'Dermatofibroma'\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "81603d003c5cea8f6e76a1bb69aa4c29a89eebc8",
        "id": "b4tQaw_Zi_N2"
      },
      "source": [
        "#3. Reading & Processing data\n",
        "\n",
        "We made some new columns which is easily understood for later reference such as the path to the image_id, cell_type which contains the short name of lesion type.\n",
        "\n",
        "We convert the categorical column cell_type_idx in which we have categorize the lesion type in to codes from 0 to 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "68f34a08751a6e16569818ce8b18d9fca93223ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UPP0YxYi_N3",
        "outputId": "bc719def-533a-4c36-c83f-29aa86319489",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "csv_name = 'HAM10000_metadata.csv'\n",
        "\n",
        "file_path = os.path.join(base_working_dir, csv_name)\n",
        "\n",
        "if not os.path.isfile(file_path):\n",
        "    url = 'https://drive.google.com/uc?authuser=0&id=1jdCVOmeJXI6bhWIfFVL7RqwyuOHiRjaE&export=download'\n",
        "    gdown.download(url, file_path, quiet=False)\n",
        "\n",
        "skin_df = pd.read_csv(file_path)\n",
        "\n",
        "# Creating New Columns for better readability\n",
        "skin_df['cell_type'] = skin_df['dx'].map(lesion_type_dict.get)\n",
        "skin_df['cell_type_idx'] = pd.Categorical(skin_df['cell_type']).codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "0de2d23671ac649a0e20f426fb891a4e8f7e903a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "85SJAhgii_N7",
        "outputId": "591d6a25-494e-40db-ce12-81110969e030",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# tile_dfnewly made columns and shape\n",
        "skin_df.head()\n",
        "\n",
        "print(skin_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "759026191d74e7d94ac08d81121913d06a053a1d",
        "id": "5N9MdkBbi_OT"
      },
      "source": [
        "#4. Exploratory Data Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "fc479dfe198cd5b781573f5c17c3a052d4fe196f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        },
        "id": "i9CwRF23i_OW",
        "outputId": "06c547a4-282c-4e2a-a693-9e9300314985",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "fig, ax1 = plt.subplots(1, 1, figsize=(10, 5))\n",
        "skin_df['cell_type'].value_counts().plot(kind='bar', ax=ax1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "574f2c782b7091305a7d921046a5a1843df09873",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "_CcoWLEqi_Ob",
        "outputId": "0732b4f8-6e11-4d04-c79f-46ef6134eeeb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "fig, ax2 = plt.subplots(1, 1, figsize=(10, 5))\n",
        "skin_df['dx_type'].value_counts().plot(kind='bar', ax=ax2)\n",
        "ax2.set_title('Distribution of Validation Methods')\n",
        "ax2.set_xlabel('Validation Method')\n",
        "ax2.set_ylabel('Count')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nValidation method counts:\")\n",
        "print(skin_df['dx_type'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "cfbcd68a065cd1abc003d14c8d3c6eb716379b8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "WNap17M8i_Oh",
        "outputId": "5d0e49ac-b8d2-45a1-829d-f5c028e9d226",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "fig, ax3 = plt.subplots(1, 1, figsize=(12, 6))\n",
        "skin_df['localization'].value_counts().plot(kind='bar', ax=ax3)\n",
        "ax3.set_title('Distribution of Cancer Location on Body')\n",
        "ax3.set_xlabel('Body Location')\n",
        "ax3.set_ylabel('Count')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTop 5 body locations:\")\n",
        "print(skin_df['localization'].value_counts().head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "6c1097e45fb83a20d8c611cd40d443194d29c517",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "Te0Ax4mGi_On",
        "outputId": "9008acea-7acb-4620-f1f8-54ced9afbb2e",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "fig, ax4 = plt.subplots(1, 1, figsize=(10, 5))\n",
        "skin_df['age'].hist(bins=30, ax=ax4, edgecolor='black')\n",
        "ax4.set_title('Age Distribution')\n",
        "ax4.set_xlabel('Age')\n",
        "ax4.set_ylabel('Count')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nAge statistics:\")\n",
        "print(skin_df['age'].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "dab9e33ef4149e65017ba09189be91323525f761",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "I-D_Td02i_Ox",
        "outputId": "1ec8b003-e793-447c-b338-1d299939aae4",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "fig, ax5 = plt.subplots(1, 1, figsize=(8, 5))\n",
        "skin_df['sex'].value_counts().plot(kind='bar', ax=ax5)\n",
        "ax5.set_title('Distribution by Sex')\n",
        "ax5.set_xlabel('Sex')\n",
        "ax5.set_ylabel('Count')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nSex distribution:\")\n",
        "print(skin_df['sex'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "247f579092e51a1424e38f8d3536badad7638c27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "J8gRmMrRi_O1",
        "outputId": "b026af72-4fb5-48ab-bb75-b889d7725c80",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Age-wise distribution of skin cancer types using scatterplot\n",
        "fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n",
        "\n",
        "sns.scatterplot(data=skin_df, x='age', y='cell_type',\n",
        "                hue='cell_type', s=50, alpha=0.6, ax=ax)\n",
        "\n",
        "ax.set_title('Age-wise Distribution of Skin Cancer Types', fontsize=14)\n",
        "ax.set_xlabel('Age', fontsize=12)\n",
        "ax.set_ylabel('Cell Type', fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title='Cell Type')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "92f3186f6f1106f3ec491d99313b506eb514f011",
        "id": "d2zi4rgZi_O_"
      },
      "source": [
        "# 5. Loading the images\n",
        "\n",
        "In the three following steps cancer images will be loaded from a shared archive to your personal Google Drive storage space. After this step a new file \"HAM10000_images.tar\" should be created in your Google Drive.\n",
        "\n",
        "1) High-resolution images and the relief of the lesion are important featuers for dermatologists to examine visually skin moles.\n",
        "\n",
        "2) We do not have access to the 3D relief using dermatoscopic images.\n",
        "\n",
        "3) We will use on downsampled images.\n",
        "\n",
        "*Interested in trying later on mid-sized images? Use the file 'HAM10000_images_300x225.tar' (you will need to download it manually to the Skin_Cancer_class directory in your Gdrive first).*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8LUgbiYcYoj"
      },
      "outputs": [],
      "source": [
        "# select below to work with small images or large images\n",
        "work_with_small_images:bool = True\n",
        "\n",
        "if work_with_small_images:\n",
        "    # use small 150x100 images first\n",
        "    url = 'https://drive.google.com/uc?authuser=0&id=1--oGquD0y48lW-6WRz5ldM1rGqbJKOez&export=download'\n",
        "    tgz_name = 'HAM10000_images_150x100.tar'\n",
        "else:\n",
        "    # high resolution is important for dermatologists to assess skin moles visually\n",
        "    # interested in training on larger images 300x250?\n",
        "    # Download this file manually to your Google drive first!\n",
        "    url = 'https://drive.google.com/uc?authuser=0&id=1-4fKAGB_rpzp6eFFEzAYOVJgXtkDzLoW&export=download'\n",
        "    tgz_name = 'HAM10000_images_300x225.tar'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE2dR-l96yMM"
      },
      "source": [
        "Download (if necessary) and load the .tar file containing compressed images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIUuhBoZVhly",
        "outputId": "6287b24a-2b77-4147-e36b-1d0a2fbca80e"
      },
      "outputs": [],
      "source": [
        "file_path = os.path.join(base_working_dir, tgz_name)\n",
        "\n",
        "if not os.path.isfile(file_path):\n",
        "    gdown.download(url, file_path, quiet=False)\n",
        "\n",
        "t = tarfile.open(os.path.join(base_working_dir, tgz_name), 'r')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQxFwWrMVypS"
      },
      "source": [
        "The images will be decompressed and loaded into the datasheet from the downloaded archive.  Loading 10000 images takes several seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "eea3ca5052d0b52b31336485258ca5f41089d980",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKEH1DFai_PB",
        "outputId": "67c23944-94ef-478e-8446-0c15b56fdbca",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# read each image and assign it as a cell content\n",
        "t1 = time()\n",
        "skin_df['image'] = skin_df['image_id'].map(lambda x: np.asarray(Image.open(io.BytesIO(t.extractfile(os.path.join('HAM10000_images',x+'.jpg')).read()))))\n",
        "print('{} images read in {} seconds.'.format(len(skin_df['image']), np.round(time()-t1, decimals=2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "dfed158580f3916f007e2208ffe35a4031e5fff9",
        "id": "iWPEbvm1i_PI"
      },
      "source": [
        "Now we have 10015 colour images of size 100x75 loaded into the computer memory.\n",
        "\n",
        "Let's check a few sample images to see each cancer type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "f56069ceb60c2f1103684ca1c024fa5c48dffb4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LgxnbE_8i_PJ",
        "outputId": "dffabb2d-150f-4721-bff4-3acf4bf521c5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "n_samples = 5\n",
        "\n",
        "fig, m_axs = plt.subplots(7, n_samples, figsize=(4*n_samples, 3*7))\n",
        "for n_axs, (type_name, type_rows) in zip(m_axs, skin_df.sort_values(['cell_type']).groupby('cell_type')):\n",
        "    n_axs[0].set_title(type_name, fontsize=14)\n",
        "    for c_ax, (_, c_row) in zip(n_axs, type_rows.sample(n_samples, random_state=1234).iterrows()):\n",
        "        image_id = c_row['image_id']\n",
        "\n",
        "        try:\n",
        "            possible_names = [\n",
        "                f'{image_id}.jpg',\n",
        "                f'HAM10000_images/{image_id}.jpg',\n",
        "                f'{image_id}.jpeg',\n",
        "                f'HAM10000_images/{image_id}.jpeg'\n",
        "            ]\n",
        "\n",
        "            image_file = None\n",
        "            for name in possible_names:\n",
        "                try:\n",
        "                    image_file = t.extractfile(name)\n",
        "                    break\n",
        "                except KeyError:\n",
        "                    continue\n",
        "\n",
        "            if image_file is None:\n",
        "                raise KeyError(f\"Image {image_id} not found\")\n",
        "\n",
        "            img = Image.open(image_file)\n",
        "\n",
        "            c_ax.imshow(img)\n",
        "            c_ax.axis('off')\n",
        "\n",
        "        except Exception as e:\n",
        "            c_ax.text(0.5, 0.5, f'Error loading\\n{image_id}',\n",
        "                     ha='center', va='center', fontsize=8)\n",
        "            c_ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "fig.savefig(os.path.join(base_working_dir,'category_samples.png'), dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "6e983d68b0a343bdf313ccb70c7cd38afd32c89b",
        "id": "Bq0JLDJji_PV"
      },
      "source": [
        "# 6. Dataset preparation\n",
        "Convert the targets (the **cell_type_idx** column of the datasheet) into the one-hot encoding format. The final data need to be PyTorch tensors with same data-type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "3fe1228e7657f49d7323adc36c865790e9ac05eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhF-jiF9i_PW",
        "outputId": "570b7301-3508-47a7-888f-814c88a154e8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "features = np.asarray(skin_df['image'].tolist())\n",
        "targets = np.asarray(skin_df['cell_type_idx'].tolist())\n",
        "\n",
        "targets_reshaped = targets.reshape(-1, 1)\n",
        "\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "targets_onehot = encoder.fit_transform(targets_reshaped)\n",
        "\n",
        "features_tensor = torch.tensor(features, dtype=torch.float32)\n",
        "targets_tensor = torch.tensor(targets_onehot, dtype=torch.float32)\n",
        "\n",
        "print(\"Features shape:\", features_tensor.shape)\n",
        "print(\"Targets shape (one-hot encoded):\", targets_tensor.shape)\n",
        "print(\"Number of classes:\", targets_onehot.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WuOWA5a6yMO",
        "outputId": "69495f36-93e7-43eb-cdc9-aa5411fcb8d8"
      },
      "outputs": [],
      "source": [
        "print(\"Original data shape: {}\".format(tuple(features_tensor.shape)))\n",
        "\n",
        "features_tensor = features_tensor.transpose(1, 3).transpose(2, 3)\n",
        "\n",
        "print(\"New data shape: {}\".format(tuple(features_tensor.shape)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOrfkIcO6yMP"
      },
      "outputs": [],
      "source": [
        "x_train_o, x_test_o, y_train, y_test = train_test_split(features_tensor, targets_tensor,\n",
        "                                                          test_size=0.2, random_state=123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "2d1c806e6c6e46916ffb40b5e2848c66c33ed719",
        "id": "sCcUy20ki_Pa"
      },
      "source": [
        "# 7. Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "cd19d9fa10edf4cd89f178db0291be76dbdcbfef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2y4NEsxi_Pe",
        "outputId": "01c46f3e-f1ed-4c24-fb38-f4d8df792e1e",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "mean = x_train_o.mean()\n",
        "std = x_train_o.std()\n",
        "\n",
        "x_train = (x_train_o - mean) / std\n",
        "x_test = (x_test_o - mean) / std\n",
        "\n",
        "print(\"Mean:\", mean.item())\n",
        "print(\"Std:\", std.item())\n",
        "print(\"Train data - Mean after normalization:\", x_train.mean().item())\n",
        "print(\"Train data - Std after normalization:\", x_train.std().item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "595ee1067d7f0c82ea07fdfec4ff9617d9d64d2c",
        "id": "p0iDpKVai_Pl"
      },
      "source": [
        "# 8. Splitting training and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "ed38171b197633a3cb7e0b1f596315456da2b2cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7QpUI-Bi_Pm",
        "outputId": "f935fb87-2baa-47de-8d18-74e8198798f8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train,\n",
        "                                                    test_size=0.1, random_state=123)\n",
        "\n",
        "print(\"Training set size:\", x_train.shape[0])\n",
        "print(\"Validation set size:\", x_val.shape[0])\n",
        "print(\"Test set size:\", x_test.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "857f705a561f046a1d63ffa17a8a0b1e8da16ff5",
        "id": "PJUE121qi_Ps"
      },
      "source": [
        "# 9. Model Building - CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKx9fU3z6yMQ"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple, Literal\n",
        "\n",
        "# Function to compute the new size of input images after being filtered (Conv2D or MaxPooling)\n",
        "def get_output_size(\n",
        "        input_size:torch.Size,\n",
        "        kernel_size:Tuple|int,\n",
        "        kernel_depth:int=1,\n",
        "        padding:Tuple|int|Literal['valid','same']='valid',\n",
        "        stride:Tuple|int=(1,1),\n",
        "        dilation:Tuple|int=(1,1)\n",
        ") -> torch.Size:\n",
        "    if type(padding) is str:\n",
        "        if padding == 'valid':\n",
        "            padding = (0,) * 2\n",
        "        elif padding == 'same':\n",
        "            padding = ((kernel_size[-2]-1)/2, (kernel_size[-1]-1)/2)\n",
        "        else: raise ValueError(\"Argument 'padding', if string, must be either 'valid' or 'same'\")\n",
        "    elif not np.iterable(padding):\n",
        "        padding = (padding,) * 2\n",
        "    if not np.iterable(stride):\n",
        "        stride = (stride,) * 2\n",
        "    if not np.iterable(dilation):\n",
        "        dilation = (dilation,) * 2\n",
        "    if not np.iterable(kernel_size):\n",
        "        kernel_size = (kernel_size,) * 2\n",
        "    new_height = int((input_size[-2] + 2 * padding[-2] - dilation[-2] * (kernel_size[-2] - 1) - 1) / stride[-2] + 1)\n",
        "    new_width_ = int((input_size[-1] + 2 * padding[-1] - dilation[-1] * (kernel_size[-1] - 1) - 1) / stride[-1] + 1)\n",
        "    return torch.Size((kernel_depth, new_height, new_width_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0uEuXg66yMQ"
      },
      "source": [
        "#### Hyperparameter set-up\n",
        "\n",
        "1. depths of convolutional kernels (conv2D_depth_.);\n",
        "2. filter kernel sizes (conv2D and maxPool);\n",
        "3. filter paddings (conv2D and maxPool);\n",
        "4. filter strides (conv2D and maxPool);\n",
        "5. size of the first dense layer (size_dense_1);\n",
        "6. dropout probabilities (2D and Dense)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bmGCNX36yMR",
        "outputId": "44c3b3b6-f982-42c7-e74c-1dc115f03a57"
      },
      "outputs": [],
      "source": [
        "# HYPERPARAMETERS\n",
        "\n",
        "input_channels = x_train.size(1)\n",
        "input_size2D = x_train.shape[2:]\n",
        "\n",
        "conv2D_depth_1 = 32\n",
        "conv2D_depth_2 = 64\n",
        "conv2D_depth_3 = 128\n",
        "conv2D_depth_4 = 256\n",
        "\n",
        "conv2D__kernel_size = (3,) * 2\n",
        "conv2D__padding = 'same'\n",
        "conv2D__stride = 1\n",
        "\n",
        "maxPool_kernel_size = (3,) * 2\n",
        "maxPool_padding = 1\n",
        "maxPool_stride = 2\n",
        "\n",
        "size_dense_1 = 512\n",
        "num_classes  = 7\n",
        "\n",
        "dropout_probability_2D = 0.25\n",
        "dropout_probability_Dense = 0.5\n",
        "\n",
        "print(\"Input channels:\", input_channels)\n",
        "print(\"Input size:\", input_size2D)\n",
        "print(\"Conv depths:\", conv2D_depth_1, conv2D_depth_2, conv2D_depth_3, conv2D_depth_4)\n",
        "print(\"Dense layer size:\", size_dense_1)\n",
        "print(\"Number of classes:\", num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KhtXOfH6yMR"
      },
      "source": [
        "#### Tensor size computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GDP97FJ6yMR",
        "outputId": "3a25fd5e-bdf4-4f6b-8f6a-8b3a2dca8963"
      },
      "outputs": [],
      "source": [
        "# Computation of the image sizes through the successive layers\n",
        "\n",
        "block_1_in_size = (input_channels, *input_size2D)\n",
        "\n",
        "block_2_in_size = get_output_size(block_1_in_size, kernel_size=conv2D__kernel_size, kernel_depth=conv2D_depth_1, padding=conv2D__padding, stride=conv2D__stride)\n",
        "block_2_in_size = get_output_size(block_2_in_size, kernel_size=conv2D__kernel_size, kernel_depth=conv2D_depth_1, padding=conv2D__padding, stride=conv2D__stride)\n",
        "block_2_in_size = get_output_size(block_2_in_size, kernel_size=maxPool_kernel_size, kernel_depth=conv2D_depth_1, padding=maxPool_padding, stride=maxPool_stride)\n",
        "\n",
        "block_3_in_size = get_output_size(block_2_in_size, kernel_size=conv2D__kernel_size, kernel_depth=conv2D_depth_2, padding=conv2D__padding, stride=conv2D__stride)\n",
        "block_3_in_size = get_output_size(block_3_in_size, kernel_size=conv2D__kernel_size, kernel_depth=conv2D_depth_2, padding=conv2D__padding, stride=conv2D__stride)\n",
        "block_3_in_size = get_output_size(block_3_in_size, kernel_size=maxPool_kernel_size, kernel_depth=conv2D_depth_2, padding=maxPool_padding, stride=maxPool_stride)\n",
        "\n",
        "block_4_in_size = get_output_size(block_3_in_size, kernel_size=conv2D__kernel_size, kernel_depth=conv2D_depth_3, padding=conv2D__padding, stride=conv2D__stride)\n",
        "block_4_in_size = get_output_size(block_4_in_size, kernel_size=conv2D__kernel_size, kernel_depth=conv2D_depth_3, padding=conv2D__padding, stride=conv2D__stride)\n",
        "block_4_in_size = get_output_size(block_4_in_size, kernel_size=maxPool_kernel_size, kernel_depth=conv2D_depth_3, padding=maxPool_padding, stride=maxPool_stride)\n",
        "\n",
        "fully_connected_in_size = get_output_size(block_4_in_size, kernel_size=conv2D__kernel_size, kernel_depth=conv2D_depth_4, padding=conv2D__padding, stride=conv2D__stride)\n",
        "fully_connected_in_size = get_output_size(fully_connected_in_size, kernel_size=conv2D__kernel_size, kernel_depth=conv2D_depth_4, padding=conv2D__padding, stride=conv2D__stride)\n",
        "fully_connected_in_size = get_output_size(fully_connected_in_size, kernel_size=maxPool_kernel_size, kernel_depth=conv2D_depth_4, padding=maxPool_padding, stride=maxPool_stride)\n",
        "\n",
        "flatten_size = fully_connected_in_size[0] * fully_connected_in_size[1] * fully_connected_in_size[2]\n",
        "\n",
        "print(\"Flatten size:\", flatten_size)\n",
        "print(\"Fully connected input size:\", fully_connected_in_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4vrIZrw6yMR"
      },
      "source": [
        "#### Building of the model\n",
        "\n",
        "This CNN architechture is\n",
        "\n",
        "$(In) \\rightarrow [[\\text{Conv2D} \\rightarrow \\text{ReLU}] \\times 2 \\rightarrow \\text{BatchNorm2D} \\rightarrow \\text{MaxPool2D} \\rightarrow \\text{Dropout}] \\times 4 \\rightarrow \\text{Flatten} \\rightarrow \\text{Dense} + \\text{Activation} \\rightarrow \\text{Dropout} \\rightarrow \\text{Dense} + \\text{Activation} ~(Out)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2SMY-Ub6yMR",
        "outputId": "c1b1f89c-9bed-4c26-bebc-64bca6fe68ec"
      },
      "outputs": [],
      "source": [
        "# Model construction\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Conv2d(in_channels=input_channels, out_channels=conv2D_depth_1, kernel_size=conv2D__kernel_size, padding=conv2D__padding, stride=conv2D__stride),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(in_channels=conv2D_depth_1, out_channels=conv2D_depth_1, kernel_size=conv2D__kernel_size, padding=conv2D__padding, stride=conv2D__stride),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(conv2D_depth_1),\n",
        "    nn.MaxPool2d(kernel_size=maxPool_kernel_size, padding=maxPool_padding, stride=maxPool_stride),\n",
        "    nn.Dropout(dropout_probability_2D),\n",
        "\n",
        "    nn.Conv2d(in_channels=conv2D_depth_1, out_channels=conv2D_depth_2, kernel_size=conv2D__kernel_size, padding=conv2D__padding, stride=conv2D__stride),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(in_channels=conv2D_depth_2, out_channels=conv2D_depth_2, kernel_size=conv2D__kernel_size, padding=conv2D__padding, stride=conv2D__stride),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(conv2D_depth_2),\n",
        "    nn.MaxPool2d(kernel_size=maxPool_kernel_size, padding=maxPool_padding, stride=maxPool_stride),\n",
        "    nn.Dropout(dropout_probability_2D),\n",
        "\n",
        "    nn.Conv2d(in_channels=conv2D_depth_2, out_channels=conv2D_depth_3, kernel_size=conv2D__kernel_size, padding=conv2D__padding, stride=conv2D__stride),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(in_channels=conv2D_depth_3, out_channels=conv2D_depth_3, kernel_size=conv2D__kernel_size, padding=conv2D__padding, stride=conv2D__stride),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(conv2D_depth_3),\n",
        "    nn.MaxPool2d(kernel_size=maxPool_kernel_size, padding=maxPool_padding, stride=maxPool_stride),\n",
        "    nn.Dropout(dropout_probability_2D),\n",
        "\n",
        "    nn.Conv2d(in_channels=conv2D_depth_3, out_channels=conv2D_depth_4, kernel_size=conv2D__kernel_size, padding=conv2D__padding, stride=conv2D__stride),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(in_channels=conv2D_depth_4, out_channels=conv2D_depth_4, kernel_size=conv2D__kernel_size, padding=conv2D__padding, stride=conv2D__stride),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(conv2D_depth_4),\n",
        "    nn.MaxPool2d(kernel_size=maxPool_kernel_size, padding=maxPool_padding, stride=maxPool_stride),\n",
        "    nn.Dropout(dropout_probability_2D),\n",
        "\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(flatten_size, size_dense_1),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(dropout_probability_Dense),\n",
        "    nn.Linear(size_dense_1, num_classes),\n",
        "    nn.Softmax(dim=1)\n",
        ")\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92TSXpkb6yMS"
      },
      "source": [
        "#### Send the model to GPU\n",
        "\n",
        "Send the model to your Cuda GPU to accelerate training, by simply using the model.cuda() function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bi63Hgh76yMS"
      },
      "outputs": [],
      "source": [
        "# Send the model to Cuda GPU\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8foigXxS6yMS"
      },
      "source": [
        "#### Model summary\n",
        "\n",
        "Print the model architecture and weights using the summary() function provided in the torchsummary library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSlmOatR6yMS",
        "outputId": "546fc16f-24bb-4fe5-a0d3-f141ccdef9f5"
      },
      "outputs": [],
      "source": [
        "%pip install torchsummary\n",
        "from torchsummary import summary\n",
        "\n",
        "summary(model, input_size=(input_channels, *input_size2D))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "e08855b5e5cad917a627b310748a10093395d8f3",
        "id": "q7C4A7ETi_Pz"
      },
      "source": [
        "# 10. Training configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "c754af8f1d34d3cfb6adb93e6824b5a5e6dc8506",
        "id": "oT_xkgfxi_P1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Define the criterion (loss function)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kbk4fSeu6yMT"
      },
      "outputs": [],
      "source": [
        "# Define the optimizer (take a relevant learning rate, e.g., lr=1e-4)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2bA8XnY6yMT"
      },
      "outputs": [],
      "source": [
        "# Define the evaluation metric\n",
        "\n",
        "# A function that returns the accuracy from two Tensor matrices: the predictions (y_hat) and the true labels (y)\n",
        "def accuracy(y_hat:torch.Tensor, y:torch.Tensor) -> float:\n",
        "    _, predicted = torch.max(y_hat, 1)\n",
        "    _, true_labels = torch.max(y, 1)\n",
        "    correct = (predicted == true_labels).sum().item()\n",
        "    total = y.size(0)\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "84be7b2fa9289e5653d779e8ecc75a5b612500d7",
        "id": "pPhObJRmi_QF"
      },
      "source": [
        "# 11. Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlapzto36yMT"
      },
      "source": [
        "Define the list of random transfom functions in a transforms.Compose() object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzxUF56K6yMT"
      },
      "outputs": [],
      "source": [
        "# Define the object for random image transforms\n",
        "\n",
        "my_transforms = transforms.Compose([\n",
        "    # random affine transform: rotation + translation + zoom\n",
        "    transforms.RandomAffine(degrees=180, translate=(0.3,)*2, scale=(0.8, 1.2)),\n",
        "    # random horizontal flip\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    # random vertical flip\n",
        "    transforms.RandomVerticalFlip(),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgeCmkye6yMT"
      },
      "source": [
        "Visualize the effect of your random image transformer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "CzwBO3pP6yMU",
        "outputId": "dcc56b5e-4d74-4f98-b115-df829000ca85"
      },
      "outputs": [],
      "source": [
        "num_img = 4\n",
        "\n",
        "def normalized(img:torch.Tensor) -> torch.Tensor:\n",
        "    return (img-img.min())/(img.max()-img.min()) if (img.max()-img.min()).abs()>1e-6 else img-img.min()\n",
        "\n",
        "img_original_show = normalized(x_train[num_img])\n",
        "img_transfor_show = my_transforms(img_original_show)\n",
        "\n",
        "_, axs = plt.subplots(1,2,figsize=(12,6))\n",
        "axs[0].imshow(img_original_show.transpose(0,1).transpose(1,2))\n",
        "axs[1].imshow(img_transfor_show.transpose(0,1).transpose(1,2))\n",
        "axs[0].set_title('Original'); axs[1].set_title('Transformed')\n",
        "axs[0].axis('off'); axs[1].axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYFdBiCV6yMU"
      },
      "source": [
        "Build a custom TensorDataset class to automatically apply image transforms when calling the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tz4-u6h6yMU"
      },
      "outputs": [],
      "source": [
        "# Custom TensorDataset class that applies data augmentation on the fly\n",
        "\n",
        "class TensorDatasetWithTransform(TensorDataset):\n",
        "    def __init__(self, x, y=None, transforms=None):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.x[idx]\n",
        "        if self.transforms is not None:\n",
        "            image = self.transforms(image)\n",
        "        if self.y is not None:\n",
        "            return image, self.y[idx]\n",
        "        return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vh-nPHSv6yMU"
      },
      "source": [
        "Define the occurrence probabilities of the classes in the train and validation datasets, to make the classes well balanced when splitted into batches, using WeightedRandomSampler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJKgUtAC6yMU"
      },
      "outputs": [],
      "source": [
        "class_counts  = y_train.sum(axis=0)\n",
        "class_weights = 1 / class_counts\n",
        "class_weights/= class_weights.sum()\n",
        "\n",
        "train_sampler = WeightedRandomSampler(weights=class_weights[y_train.argmax(dim=-1)], num_samples=len(y_train), replacement=True)\n",
        "val_sampler   = WeightedRandomSampler(weights=class_weights[y_val  .argmax(dim=-1)], num_samples=len(y_val  ), replacement=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLCCOMza6yMV"
      },
      "source": [
        "Put train and validation datasets in two custom TensorDataset with our transformer object, and split them into batches using DataLoader and weighted samplers as argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFllr_ZT6yMV",
        "outputId": "42227719-8d69-446f-f96c-8ff6ce50193d"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(TensorDatasetWithTransform(x_train, y_train, my_transforms), batch_size=batch_size, sampler=train_sampler)\n",
        "val_loader   = DataLoader(TensorDatasetWithTransform(x_val,   y_val,   my_transforms), batch_size=batch_size, sampler=val_sampler)\n",
        "test_loader  = DataLoader(TensorDatasetWithTransform(x_test,  y_test), batch_size=batch_size)\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Validation batches: {len(val_loader)}\")\n",
        "print(f\"Test batches: {len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "e7644b4ef037b8e051466691e2e2197364be43b2",
        "id": "DK8YIjDMi_QN"
      },
      "source": [
        "# 12. Training the model\n",
        "\n",
        "A 50-epoch training lasts around 16 minutes.\n",
        "\n",
        "You can also allow training just for 20 epochs. In 20 epochs (~7minutes) your model should almost converge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "or-hXZ1T6yMV",
        "outputId": "09496910-9045-4289-a58a-505747873e8c"
      },
      "outputs": [],
      "source": [
        "epochs = 50\n",
        "patience = 10\n",
        "\n",
        "save_path = os.path.join(base_working_dir, 'model_best.pth')\n",
        "\n",
        "best_val_acc = 0.0\n",
        "epochs_no_improve = 0\n",
        "early_stop = False\n",
        "\n",
        "train_losses, val_losses = [], []\n",
        "train_accs, val_accs = [], []\n",
        "\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "train_loop_verbose = True\n",
        "early_stop_verbose = False\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    if train_loop_verbose:\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "\n",
        "    model.train()\n",
        "    train_loss, train_acc = 0.0, 0.0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "        train_acc  += accuracy(outputs, labels) * inputs.size(0)\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_acc  /= len(train_loader.dataset)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss, val_acc = 0.0, 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            val_acc  += accuracy(outputs, labels) * inputs.size(0)\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    val_acc  /= len(val_loader.dataset)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "    if train_loop_verbose:\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        if early_stop_verbose:\n",
        "            print(f\"OK: Model improved and saved to {save_path}\")\n",
        "        epochs_no_improve = 0\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if early_stop_verbose:\n",
        "            print(f\"No improvement for {epochs_no_improve} epochs.\")\n",
        "\n",
        "    if epochs_no_improve >= patience:\n",
        "        if early_stop_verbose:\n",
        "            print(f\"ES: Early stopping at epoch {epoch+1}\")\n",
        "        early_stop = True\n",
        "        break\n",
        "\n",
        "if early_stop_verbose and not early_stop:\n",
        "    print(\"Training completed without early stopping.\")\n",
        "\n",
        "model.load_state_dict(best_model_wts)\n",
        "print(f\"Best validation accuracy: {best_val_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "6d4fc72b2aa6fb8848c51257affb6fb872efb725",
        "id": "DBaKsd5Ti_QT"
      },
      "source": [
        "# 13. Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "7b1b665dacff5521aa9d5c2c0926119a3829d623",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "39nFI-85i_QW",
        "outputId": "2ad929c2-62d4-4710-b7c0-d43513490161",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Summarize training history for loss and accuracy\n",
        "history = {\n",
        "    'train_loss': train_losses,\n",
        "    'val_loss': val_losses,\n",
        "    'train_acc': train_accs,\n",
        "    'val_acc': val_accs\n",
        "}\n",
        "\n",
        "plot_training_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgAJ4TrsTtcd"
      },
      "source": [
        "### **Evaluate the model on the test dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bL3gweWR6yMW",
        "outputId": "90618008-ac9c-40be-d99c-36cf0afc0006"
      },
      "outputs": [],
      "source": [
        "# Predict the values from the test dataset\n",
        "model.eval()\n",
        "y_pred_list = []\n",
        "y_true_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs = inputs.cuda()\n",
        "        outputs = model(inputs)\n",
        "        y_pred_list.append(outputs.cpu())\n",
        "        y_true_list.append(labels)\n",
        "\n",
        "y_pred = torch.cat(y_pred_list, dim=0)\n",
        "y_true = torch.cat(y_true_list, dim=0)\n",
        "\n",
        "test_acc = accuracy(y_pred, y_true)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcbZ4ku56yMW"
      },
      "source": [
        "Compute the loss and accuracy of the model on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2qE7eVy6yMW",
        "outputId": "ad48e0c0-24bf-4b56-cfac-4779de556126"
      },
      "outputs": [],
      "source": [
        "test_loss = criterion(y_pred, y_true).item()\n",
        "test_acc = accuracy(y_pred, y_true)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z98T0WLl6yMX"
      },
      "source": [
        "Plot the confusion matrix and check the missclassified count of each type.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AtZvhnWTSexj",
        "outputId": "017ec4c6-cbb5-4aa7-bdff-5a5267dcac21"
      },
      "outputs": [],
      "source": [
        "y_pred_classes = y_pred.argmax(dim=1)\n",
        "y_true_classes = y_true.argmax(dim=1)\n",
        "\n",
        "confusion_mtx = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "\n",
        "cell_types = ['Melanocytic nevi', 'Melanoma', 'Benign keratosis-like lesions',\n",
        "              'Basal cell carcinoma', 'Actinic keratoses', 'Vascular lesions', 'Dermatofibroma']\n",
        "\n",
        "plot_confusion_matrix(y_true_classes, y_pred_classes, cell_types)\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_mtx)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MISCLASSIFICATION ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for i, cell_type in enumerate(cell_types):\n",
        "    total = confusion_mtx[i].sum()\n",
        "    correct = confusion_mtx[i, i]\n",
        "    misclassified = total - correct\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "\n",
        "    print(f\"\\n{cell_type}:\")\n",
        "    print(f\"  Total samples: {total}\")\n",
        "    print(f\"  Correctly classified: {correct}\")\n",
        "    print(f\"  Misclassified: {misclassified}\")\n",
        "    print(f\"  Accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "total_samples = confusion_mtx.sum()\n",
        "total_correct = confusion_mtx.diagonal().sum()\n",
        "total_misclassified = total_samples - total_correct\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"OVERALL: {total_correct}/{total_samples} correct, {total_misclassified} misclassified\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "4db6351f7614f4be476a985ed5eeab24943f36dc",
        "id": "4NyU5ExAi_Qb"
      },
      "source": [
        "Now, lets see which category has most incorrect predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "e8499e6c3506c816ea86ca97c8461d5c040b6a3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "YvdZyr5_i_Qc",
        "outputId": "b7cd391f-dac5-4fdf-9289-d2f001c4b8a3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "label_frac_error = 1 - np.diag(confusion_mtx) / np.sum(confusion_mtx, axis=1)\n",
        "\n",
        "plt.bar(np.arange(y_test.size(-1)),label_frac_error)\n",
        "plt.xlabel('True Label')\n",
        "plt.ylabel('Fraction classified incorrectly')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
