{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e0e377f8-e766-4171-8b4e-e14b0b8c6e4e",
      "metadata": {
        "id": "e0e377f8-e766-4171-8b4e-e14b0b8c6e4e"
      },
      "source": [
        "# LSTM model for Sequential Medical Data & RNNs comparison\n",
        "\n",
        "### Our Goal:\n",
        "\n",
        "1.  **Build a sequential patient history**: Instead of one averaged vector per patient, we will create a *sequence of vectors* for each patient, where each vector represents a single hospital visit.\n",
        "2.  **Use a Recurrent Neural Network (RNN)**: We will feed this 3D data `(patients, timesteps, features)` into an **LSTM** (Long Short-Term Memory) network.\n",
        "3.  **Compare Performance**: We will show that thios sequence-aware model outperform previous \"bag-of-words\" models for the 1-year mortality prediction task and compare it with GRU and Bidirectional LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fOEeMMgq1y3",
      "metadata": {
        "id": "7fOEeMMgq1y3"
      },
      "source": [
        "# ðŸ“‚ **Data Access & Setup**\n",
        "\n",
        "This project uses the **MIMIC-IV v3.1** dataset. Due to the sensitive nature of clinical data and regulation, the raw datasets are not included in this repository.\n",
        "\n",
        "**1. Requesting Access**\n",
        "To run this pipeline, you must have a signed Data Use Agreement (DUA):\n",
        "\n",
        "Training: Complete the [CITI Data or Specimens Researchers training](https://about.citiprogram.org/)\n",
        "\n",
        "PhysioNet: Create an account and request access via the [MIMIC-IV PhysioNet Page](https://physionet.org/content/mimiciv/3.1/)\n",
        "\n",
        "**2. Local Setup**\n",
        "Once access is granted, download the following files and place them in a folder named data/ in the root of this project:\n",
        "\n",
        "hosp/patients.csv.gz\n",
        "\n",
        "hosp/admissions.csv.gz\n",
        "\n",
        "hosp/diagnoses_icd.csv.gz\n",
        "\n",
        "hosp/procedures_icd.csv.gz\n",
        "\n",
        "icu/icustays.csv.gz\n",
        "\n",
        "**3. Running on Google Colab**\n",
        "If using Google Colab, upload these files to your Google Drive and update the data_path variable at the top of the notebook to point to your Drive folder.\n",
        "\n",
        "\n",
        "- - -\n",
        "\n",
        "\n",
        "We use the **Med2Vec model built in the last notebook** called 'Word2Vec for Clinical Codes Embedding & Logistic Regression 1-year mortality prediction'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0bb8a7e-1216-4158-9a4f-5612ac6e36d4",
      "metadata": {
        "id": "c0bb8a7e-1216-4158-9a4f-5612ac6e36d4"
      },
      "source": [
        "## Setup: Import Libraries\n",
        "\n",
        "We will need `tensorflow` and `keras` to build our LSTM. You will also need `gensim` to load the model from the previous notebook (Word2Vec for Clinical Codes Embedding & Logistic Regression 1-year mortality prediction)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZdpAz-SnkPTZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdpAz-SnkPTZ",
        "outputId": "b2d3f11e-23d7-48cc-fd46-582aa3a5f19a"
      },
      "outputs": [],
      "source": [
        "%pip install scipy tensorflow gensim scikit-learn pandas numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e5d9c73-ec8c-4f7f-85d9-4876b539423c",
      "metadata": {
        "id": "2e5d9c73-ec8c-4f7f-85d9-4876b539423c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "import time\n",
        "# To load our pre-trained embeddings\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# For splitting and class weights\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# For building the RNN\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, LSTM, GRU, Dense, Dropout, Masking, Bidirectional\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.metrics import AUC\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Data\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40902c11-92be-4db5-ac3f-0ea2bc40dd68",
      "metadata": {
        "id": "40902c11-92be-4db5-ac3f-0ea2bc40dd68"
      },
      "source": [
        "## Part 1: Load our Med2vec Model and Define Cohort\n",
        "\n",
        "First, we load the `med2vec_model.wv` we trained in the previous notebook (Word2Vec for Clinical Codes Embedding & Logistic Regression 1-year mortality prediction). We will use this as our embedding lookup.\n",
        "\n",
        "Second, we will re-build our 1-year mortality cohort, exactly as we did in the previous notebooks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0c4064d-df81-43ee-8fa1-e63d3f9e9921",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0c4064d-df81-43ee-8fa1-e63d3f9e9921",
        "outputId": "0a110334-01cb-4685-82e9-c026ed7cdd4c"
      },
      "outputs": [],
      "source": [
        "data_path = '' # write your data path\n",
        "\n",
        "EMBEDDING_MODEL_PATH = \"med2vec_mimic4.w2v\" # From previous notebook\n",
        "\n",
        "# Load Pre-trained Embeddings\n",
        "try:\n",
        "    med2vec_model = Word2Vec.load(EMBEDDING_MODEL_PATH)\n",
        "    embedding_dim = med2vec_model.vector_size\n",
        "    model_vocab = set(med2vec_model.wv.key_to_index.keys())\n",
        "    print(f\"Loaded Med2Vec model with {len(model_vocab)} codes and {embedding_dim}-dim vectors.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: '{EMBEDDING_MODEL_PATH}' not found. Please run previsou lab.\")\n",
        "    # Create dummy vars to allow notebook to be read\n",
        "    med2vec_model = None\n",
        "    embedding_dim = 100\n",
        "    model_vocab = set()\n",
        "\n",
        "# Load and Define Cohort\n",
        "print(\"Rebuilding cohort from previsous lab...\")\n",
        "patients = pd.read_csv(os.path.join(data_path,'patients.csv.gz'))\n",
        "admissions = pd.read_csv(os.path.join(data_path,'admissions.csv.gz'))\n",
        "icustays = pd.read_csv(os.path.join(data_path,'icustays.csv.gz'))\n",
        "\n",
        "patients['dod'] = pd.to_datetime(patients['dod'])\n",
        "admissions['admittime'] = pd.to_datetime(admissions['admittime'])\n",
        "admissions['dischtime'] = pd.to_datetime(admissions['dischtime'])\n",
        "icustays['intime'] = pd.to_datetime(icustays['intime'])\n",
        "icustays['outtime'] = pd.to_datetime(icustays['outtime'])\n",
        "\n",
        "admissions = admissions.merge(patients[['subject_id', 'anchor_age', 'anchor_year', 'dod']], on='subject_id')\n",
        "admissions['age_at_admission'] = admissions['anchor_age'] + (admissions['admittime'].dt.year - admissions['anchor_year'])\n",
        "adult_admissions = admissions[admissions['age_at_admission'] >= 18]\n",
        "adult_icu = icustays.merge(adult_admissions, on=['subject_id', 'hadm_id'])\n",
        "adult_icu = adult_icu.sort_values(by='intime')\n",
        "first_icu_stays = adult_icu.groupby('subject_id').first().reset_index()\n",
        "cohort = first_icu_stays[first_icu_stays['deathtime'].isnull()].copy()\n",
        "cohort[\"index_date\"] = cohort['outtime']\n",
        "\n",
        "cohort[\"time_to_death\"] = (cohort['dod'] - cohort['index_date']).dt.days\n",
        "cohort['label_1yr_mortality'] = (cohort[\"time_to_death\"] <= 365) & (cohort[\"time_to_death\"] > 0)\n",
        "\n",
        "# This is our final cohort: subject_id and their label\n",
        "final_cohort = cohort[['subject_id', 'label_1yr_mortality']].set_index('subject_id')\n",
        "\n",
        "print(f\"Cohort of {len(final_cohort)} patients recreated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26d6a2f4-7e5f-4d9d-afc2-7b00da0d16ed",
      "metadata": {
        "id": "26d6a2f4-7e5f-4d9d-afc2-7b00da0d16ed"
      },
      "source": [
        "## Part 2: Building Sequential Data\n",
        "\n",
        "This is the most important new step. We will create patient histories as a **sequence of visits**.\n",
        "\n",
        "1.  Load all codes (diag, proc, presc) for *all* admissions.\n",
        "2.  Create a \"visit vector\" for *every* admission (`hadm_id`) by averaging the embeddings of all codes in that visit (just like we did for the *patient* vector in the previous lab).\n",
        "3.  Find all hospital admissions for the patients in our cohort, *sorted by time*.\n",
        "4.  Group these ordered visits by patient to create our final sequences (a list of visit vectors for each patient)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3d63d8f",
      "metadata": {
        "id": "b3d63d8f",
        "outputId": "c6e65530-58ec-46c5-db89-32ab363d198e"
      },
      "outputs": [],
      "source": [
        "# Load all features\n",
        "print(\"Loading all features...\")\n",
        "diagnoses = pd.read_csv(os.path.join(data_path,'diagnoses_icd.csv.gz'))\n",
        "procedures = pd.read_csv(os.path.join(data_path,'procedures_icd.csv.gz'))\n",
        "prescriptions = pd.read_csv(os.path.join(data_path,'prescriptions.csv.gz'))\n",
        "\n",
        "diag_features = diagnoses[['hadm_id', 'icd_code']].rename(columns={'icd_code':'feature'})\n",
        "proc_features = procedures[['hadm_id', 'icd_code']].rename(columns={'icd_code':'feature'})\n",
        "presc_features = prescriptions[['hadm_id', 'drug']].rename(columns={'drug':'feature'})\n",
        "\n",
        "all_features = pd.concat([diag_features, proc_features, presc_features]).dropna()\n",
        "\n",
        "# Create Visit Vectors for ALL hadm_ids\n",
        "print(\"Building visit-level corpus...\")\n",
        "# Group all codes by hadm_id\n",
        "visit_corpus = all_features.groupby('hadm_id')['feature'].apply(list)\n",
        "\n",
        "# Function to average embeddings for a list of codes\n",
        "def get_avg_vector(codes, model_wv, vocab, dim):\n",
        "    vec = np.zeros(dim)\n",
        "    count = 0\n",
        "    if isinstance(codes, list):\n",
        "        for code in codes:\n",
        "            if code in vocab:\n",
        "                vec += model_wv.wv[code]\n",
        "                count += 1\n",
        "    if count > 0:\n",
        "        vec /= count\n",
        "    return vec\n",
        "\n",
        "print(\"Calculating average embedding for every visit...\")\n",
        "visit_vectors = visit_corpus.progress_apply(\n",
        "    get_avg_vector,\n",
        "    args=(med2vec_model, model_vocab, embedding_dim)\n",
        ")\n",
        "\n",
        "# We now have a mapping: {hadm_id: [0.1, -0.4, ...], ...}\n",
        "visit_vector_map = dict(zip(visit_vectors.index, visit_vectors.values))\n",
        "print(\"Visit vector map created.\")\n",
        "\n",
        "# Load the dictionary from the file\n",
        "with open('visit_vector_map.pkl', 'rb') as f:\n",
        "    visit_vector_map = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceb6ecb0",
      "metadata": {
        "id": "ceb6ecb0",
        "outputId": "2846ab6f-1a0b-40f8-fe4c-19881390e63e"
      },
      "outputs": [],
      "source": [
        "# Save the dictionary to a file\n",
        "with open('visit_vector_map.pkl', 'wb') as f:\n",
        "    pickle.dump(visit_vector_map, f)\n",
        "\n",
        "print(\"âœ… Visit vectors saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc9eb2ee-52e2-4120-a370-28f3a79f0dd5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "fc9eb2ee-52e2-4120-a370-28f3a79f0dd5",
        "outputId": "75d525d9-b135-4acd-e63f-424970ef31b9"
      },
      "outputs": [],
      "source": [
        "# Get Ordered Visit Sequences for Cohort Patients\n",
        "print(\"Building ordered patient sequences...\")\n",
        "# Get all admissions for the patients in our cohort\n",
        "cohort_subjects = final_cohort.index.unique()\n",
        "all_cohort_admissions = admissions[\n",
        "    admissions['subject_id'].isin(cohort_subjects)\n",
        "][['subject_id', 'hadm_id', 'admittime']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e818817b-832f-4c5e-859a-14d4857ed8e5",
      "metadata": {
        "id": "e818817b-832f-4c5e-859a-14d4857ed8e5",
        "outputId": "ee43eb31-e560-4ea1-fda6-417de717244f"
      },
      "outputs": [],
      "source": [
        "# Sort by patient and then by admission time\n",
        "all_cohort_admissions = all_cohort_admissions.sort_values(by=['subject_id', 'admittime'])\n",
        "\n",
        "# Group by subject_id to get the list of ordered hadm_ids\n",
        "ordered_hadm_sequences = all_cohort_admissions.groupby('subject_id')['hadm_id'].apply(list)\n",
        "\n",
        "# Create Final (X, y) Data\n",
        "print(\"Mapping visit vectors to patient sequences...\")\n",
        "X_sequences = []\n",
        "y_labels = []\n",
        "default_vector = np.zeros(embedding_dim)\n",
        "\n",
        "# Iterate through our ordered patient sequences\n",
        "for subject_id, hadm_ids in tqdm(ordered_hadm_sequences.items(), desc=\"Mapping sequences\"):\n",
        "    patient_sequence = []\n",
        "    for hadm_id in hadm_ids:\n",
        "        # Get the pre-calculated vector for that visit\n",
        "        # Use a default (zero) vector if hadm_id had no valid codes\n",
        "        visit_vec = visit_vector_map.get(hadm_id, default_vector)\n",
        "        patient_sequence.append(visit_vec)\n",
        "\n",
        "    # Only add if the patient is in our final (labeled) cohort\n",
        "    if subject_id in final_cohort.index:\n",
        "        X_sequences.append(patient_sequence)\n",
        "        y_labels.append(final_cohort.loc[subject_id]['label_1yr_mortality'])\n",
        "\n",
        "# 'X_sequences' is a list of lists of vectors. 'y_labels' is a list of 0s and 1s.\n",
        "y = np.array(y_labels).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4f6331a",
      "metadata": {
        "id": "c4f6331a",
        "outputId": "8f801cb5-9375-4aba-ca40-28896d13e8d3"
      },
      "outputs": [],
      "source": [
        "# Calculate sequence lengths\n",
        "sequence_lengths = [len(seq) for seq in X_sequences]\n",
        "\n",
        "# Statistics\n",
        "min_visits = min(sequence_lengths)\n",
        "max_visits = max(sequence_lengths)\n",
        "mean_visits = np.mean(sequence_lengths)\n",
        "median_visits = np.median(sequence_lengths)\n",
        "\n",
        "print(f\"Minimum number of visits: {min_visits}\")\n",
        "print(f\"Maximum number of visits: {max_visits}\")\n",
        "print(f\"Mean: {mean_visits:.2f}\")\n",
        "print(f\"Median: {median_visits:.1f}\")\n",
        "\n",
        "# Distribution\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.hist(sequence_lengths, bins=50, color='steelblue', edgecolor='black')\n",
        "plt.axvline(mean_visits, color='red', linestyle='--', label=f'Mean: {mean_visits:.1f}')\n",
        "plt.xlabel('Number of Visits')\n",
        "plt.ylabel('Number of Patients')\n",
        "plt.title('Distribution of Visit Sequences per Patient')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6212e3e-ab71-4ef4-9133-722165c71b69",
      "metadata": {
        "id": "c6212e3e-ab71-4ef4-9133-722165c71b69"
      },
      "source": [
        "## Part 3: Padding the Sequences\n",
        "\n",
        "As we see above, our patient sequences have different lengths. To train an RNN in a batch, Keras requires all sequences in a batch to have the **same length**.\n",
        "\n",
        "We use `pad_sequences` to fix this.\n",
        "* It will find the longest sequence in the dataset (e.g., 50 visits).\n",
        "* It will pad all shorter sequences with zero-vectors at the **beginning** (this is `padding='pre'`).\n",
        "\n",
        "Our final data `X` will be a 3D NumPy array of shape:\n",
        "**(num_patients, max_sequence_length, num_features)**\n",
        "e.g., `(45000, 50, 100)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffab7208-d1d7-4bc5-a0c4-a550fa5df525",
      "metadata": {
        "id": "ffab7208-d1d7-4bc5-a0c4-a550fa5df525",
        "outputId": "412681c9-f5b8-4b63-f348-dd2ad6a9811e"
      },
      "outputs": [],
      "source": [
        "X_padded = pad_sequences(\n",
        "    X_sequences,\n",
        "    padding='pre',  # Pad at the beginning of the sequence\n",
        "    truncating='pre', # Truncate from the beginning if too long\n",
        "    dtype='float32' # Keras needs a consistent float type\n",
        ")\n",
        "\n",
        "print(f\"Shape of X after padding: {X_padded.shape}\")\n",
        "\n",
        "X = X_padded\n",
        "# 'y' is already our aligned NumPy array of labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b73ed21f-9da3-42e8-9642-e0691efdf4c4",
      "metadata": {
        "id": "b73ed21f-9da3-42e8-9642-e0691efdf4c4"
      },
      "source": [
        "## Part 4: Building the LSTM Model\n",
        "\n",
        "Now we build our sequence model. We will use a `Sequential` Keras model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8334466-9ab7-4a0d-88f5-410a08e1e793",
      "metadata": {
        "id": "e8334466-9ab7-4a0d-88f5-410a08e1e793",
        "outputId": "a4d51e79-ef97-4330-813c-38a29c6d1e47"
      },
      "outputs": [],
      "source": [
        "# Get the shape of our input\n",
        "# X.shape[1] = max_sequence_length (e.g., 50)\n",
        "# X.shape[2] = n_features (e.g., 100)\n",
        "input_shape = (X.shape[1], X.shape[2])\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# 1. Input Layer: Tell the model the shape of the data\n",
        "model.add(Input(shape=input_shape))\n",
        "\n",
        "# 2. Masking Layer: Ignore padding (0.0 values)\n",
        "# This is critical for padded sequences!\n",
        "model.add(Masking(mask_value=0.0))\n",
        "\n",
        "# 3. LSTM Layer: The 'memory' of our network\n",
        "model.add(LSTM(64)) # 64-unit memory cell\n",
        "\n",
        "# 4. Dropout Layer: For regularization\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# 5. Output Layer: Final prediction\n",
        "model.add(Dense(1, activation='sigmoid')) # Sigmoid for binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[AUC(name='auroc'),AUC(curve='PR',  name='auprc')] # Area Under ROC Curve\n",
        ")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5c1e550-fc89-49dd-b21a-dd69231f4a47",
      "metadata": {
        "id": "e5c1e550-fc89-49dd-b21a-dd69231f4a47"
      },
      "source": [
        "## Part 5: Training & Evaluating the Model\n",
        "\n",
        "Now we're ready to train. We will split our 3D data `X` and labels `y` into training and testing sets.\n",
        "\n",
        "We also need to handle the **class imbalance** (from TP 3, we know far more patients survive). We'll use `class_weight` to tell the model to pay more attention to the rare 'Mortality' class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e5680b0",
      "metadata": {
        "id": "1e5680b0"
      },
      "outputs": [],
      "source": [
        "# LIMITING SEQ LENGHT FOR COMPUTATION REASONS\n",
        "# Most patients have < 20 visits, so truncate at 20\n",
        "X_padded = pad_sequences(\n",
        "    X_sequences,\n",
        "    maxlen=20,  # ADD THIS\n",
        "    padding='pre',\n",
        "    truncating='pre',\n",
        "    dtype='float32'\n",
        ")\n",
        "\n",
        "X = X_padded\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0387b92f-5d07-4e76-88d4-ce36ba33b664",
      "metadata": {
        "id": "0387b92f-5d07-4e76-88d4-ce36ba33b664",
        "outputId": "78d54b33-098e-4513-cc57-6568deade8da"
      },
      "outputs": [],
      "source": [
        "# 1. Split the data\n",
        "patient_ids = np.array(ordered_hadm_sequences.index)\n",
        "\n",
        "X_train, X_test, y_train, y_test, id_train, id_test = train_test_split(\n",
        "    X, y, patient_ids,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "print(f\"Train data shape: {X_train.shape}\")\n",
        "print(f\"Test data shape: {X_test.shape}\")\n",
        "\n",
        "# 2. Calculate class weights to handle imbalance\n",
        "weights = class_weight.compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "class_weights = {0: weights[0], 1: weights[1]}\n",
        "print(f\"Class Weights: {class_weights}\")\n",
        "\n",
        "# 3. Train the model\n",
        "print(\"\\nTraining LSTM model...\")\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_split=0.2, # Use 20% of training data for validation\n",
        "    epochs=10,            # Start with 10 passes over the data\n",
        "    batch_size=64,\n",
        "    class_weight=class_weights,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Model training complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0714ed4-e0b6-455c-bc3b-74dbb8a4f91e",
      "metadata": {
        "id": "d0714ed4-e0b6-455c-bc3b-74dbb8a4f91e",
        "outputId": "a2f11525-9978-45fc-a0df-92ab17fc9229"
      },
      "outputs": [],
      "source": [
        "# 4. Plot training history\n",
        "\n",
        "print(\"Plotting training history...\")\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(history.history['auroc'], label='Train AUROC')\n",
        "plt.plot(history.history['val_auroc'], label='Validation AUROC')\n",
        "plt.title('Model AUROC')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('AUROC')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(history.history['auprc'], label='Train AUPRC')\n",
        "plt.plot(history.history['val_auprc'], label='Validation AUPRC')\n",
        "plt.title('Model AUPRC')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('AUPRC')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 5. Evaluate on the held-out test set\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "results = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {results[0]:.4f}\")\n",
        "print(f\"Test AUROC: {results[1]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57ecacff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "57ecacff",
        "outputId": "5cc2f9d5-65c0-492c-daa9-1429f2a8ba24"
      },
      "outputs": [],
      "source": [
        "# COMPARE MODEL with GRU and Bidirectional LSTM\n",
        "\n",
        "# Store results\n",
        "results_comparison = []\n",
        "\n",
        "# ============================================================\n",
        "# MODEL 2: GRU\n",
        "# ============================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"MODEL 2: GRU\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "model_gru = Sequential([\n",
        "    Input(shape=(X.shape[1], X.shape[2])),\n",
        "    Masking(mask_value=0.0),\n",
        "    GRU(64),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_gru.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[AUC(name='auroc'), AUC(curve='PR', name='auprc')]\n",
        ")\n",
        "\n",
        "print(f\"Parameters: {model_gru.count_params():,}\")\n",
        "\n",
        "start_time = time.time()\n",
        "history_gru = model_gru.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    class_weight=class_weights,\n",
        "    verbose=0\n",
        ")\n",
        "train_time_gru = time.time() - start_time\n",
        "\n",
        "test_results_gru = model_gru.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "results_comparison.append({\n",
        "    'Model': 'GRU',\n",
        "    'Parameters': model_gru.count_params(),\n",
        "    'Train Time (s)': train_time_gru,\n",
        "    'Test Loss': test_results_gru[0],\n",
        "    'Test AUROC': test_results_gru[1],\n",
        "    'Test AUPRC': test_results_gru[2]\n",
        "})\n",
        "\n",
        "print(f\"Training time: {train_time_gru:.1f}s\")\n",
        "print(f\"Test AUROC: {test_results_gru[1]:.4f}\")\n",
        "print(f\"Test AUPRC: {test_results_gru[2]:.4f}\\n\")\n",
        "\n",
        "# ============================================================\n",
        "# MODEL 3: Bidirectional LSTM\n",
        "# ============================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"MODEL 3: Bidirectional LSTM\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "model_bilstm = Sequential([\n",
        "    Input(shape=(X.shape[1], X.shape[2])),\n",
        "    Masking(mask_value=0.0),\n",
        "    Bidirectional(LSTM(64)),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_bilstm.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[AUC(name='auroc'), AUC(curve='PR', name='auprc')]\n",
        ")\n",
        "\n",
        "print(f\"Parameters: {model_bilstm.count_params():,}\")\n",
        "\n",
        "start_time = time.time()\n",
        "history_bilstm = model_bilstm.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    class_weight=class_weights,\n",
        "    verbose=0\n",
        ")\n",
        "train_time_bilstm = time.time() - start_time\n",
        "\n",
        "test_results_bilstm = model_bilstm.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "results_comparison.append({\n",
        "    'Model': 'Bidirectional LSTM',\n",
        "    'Parameters': model_bilstm.count_params(),\n",
        "    'Train Time (s)': train_time_bilstm,\n",
        "    'Test Loss': test_results_bilstm[0],\n",
        "    'Test AUROC': test_results_bilstm[1],\n",
        "    'Test AUPRC': test_results_bilstm[2]\n",
        "})\n",
        "\n",
        "print(f\"Training time: {train_time_bilstm:.1f}s\")\n",
        "print(f\"Test AUROC: {test_results_bilstm[1]:.4f}\")\n",
        "print(f\"Test AUPRC: {test_results_bilstm[2]:.4f}\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f2e1f59",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "3f2e1f59",
        "outputId": "189d442a-fc2a-4499-c892-2d1f1eeeef6b"
      },
      "outputs": [],
      "source": [
        "# Get predictions on test set\n",
        "y_pred_probs = model.predict(X_test).flatten()\n",
        "\n",
        "# Convert probabilities to binary predictions (threshold = 0.5)\n",
        "y_pred = (y_pred_probs >= 0.5).astype(int)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Extract values\n",
        "TN, FP, FN, TP = cm.ravel()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CONFUSION MATRIX ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nTrue Negatives (TN):  {TN:>6} - Correctly predicted survivors\")\n",
        "print(f\"False Positives (FP): {FP:>6} - Predicted death, actually survived\")\n",
        "print(f\"False Negatives (FN): {FN:>6} - Predicted survival, actually died\")\n",
        "print(f\"True Positives (TP):  {TP:>6} - Correctly predicted deaths\")\n",
        "\n",
        "print(f\"\\n{'â”€' * 60}\")\n",
        "print(f\"Total False Positives:  {FP}\")\n",
        "print(f\"Total False Negatives:  {FN}\")\n",
        "print(f\"{'â”€' * 60}\")\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "print(f\"\\nPERFORMANCE METRICS:\")\n",
        "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"  Precision: {precision:.4f} (of predicted deaths, {precision*100:.1f}% were correct)\")\n",
        "print(f\"  Recall:    {recall:.4f} (detected {recall*100:.1f}% of actual deaths)\")\n",
        "print(f\"  F1-Score:  {f1:.4f}\")\n",
        "\n",
        "# Visualize confusion matrix\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Heatmap with counts\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Survived', 'Died'],\n",
        "            yticklabels=['Survived', 'Died'],\n",
        "            ax=axes[0], cbar=False)\n",
        "axes[0].set_xlabel('Predicted Label', fontsize=12)\n",
        "axes[0].set_ylabel('True Label', fontsize=12)\n",
        "axes[0].set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Normalized heatmap\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',\n",
        "            xticklabels=['Survived', 'Died'],\n",
        "            yticklabels=['Survived', 'Died'],\n",
        "            ax=axes[1], cbar=False)\n",
        "axes[1].set_xlabel('Predicted Label', fontsize=12)\n",
        "axes[1].set_ylabel('True Label', fontsize=12)\n",
        "axes[1].set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"DETAILED CLASSIFICATION REPORT\")\n",
        "print(\"=\" * 60)\n",
        "print(classification_report(y_test, y_pred,\n",
        "                          target_names=['Survived (0)', 'Died (1)'],\n",
        "                          digits=4))\n",
        "\n",
        "# Clinical interpretation\n",
        "print(\"=\" * 60)\n",
        "print(\"CLINICAL INTERPRETATION\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nFalse Positives ({FP} patients):\")\n",
        "print(\"  â†’ Predicted to die but survived\")\n",
        "print(\"  â†’ Clinical impact: Unnecessary interventions, patient anxiety\")\n",
        "print(f\"\\nFalse Negatives ({FN} patients):\")\n",
        "print(\"  â†’ Predicted to survive but died\")\n",
        "print(\"  â†’ Clinical impact: CRITICAL - Missed high-risk patients!\")\n",
        "print(\"\\n\" + \"=\" * 60)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
